{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1155b4a4",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "Resume parsing is a valuable tool used in various real-life scenarios to simplify and streamline the hiring process. Imagine you're a busy hiring manager or a human resources professional responsible for reviewing countless resumes. It can be quite overwhelming and time-consuming to manually read through each document and extract the relevant information. This is where a resume parser comes in.\n",
    "\n",
    "A resume parser is like a smart assistant that helps automate the initial screening of resumes. It uses advanced algorithms and natural language processing techniques to analyze the content of a resume and extract key details such as contact information, education history, work experience, skills, and more. This information is then organized into a structured format, making it easier for recruiters to evaluate candidates efficiently.\n",
    "\n",
    "With a resume parser, you can quickly scan through a large pool of resumes and identify the most suitable candidates based on specific criteria. It allows you to search for particular skills, experience levels, educational backgrounds, or any other qualifications you require for the job. The parser also eliminates the possibility of human error and ensures consistent and accurate data extraction.\n",
    "\n",
    "Furthermore, resume parsing can be integrated with applicant tracking systems (ATS) or other recruitment software. This integration enables seamless data transfer and eliminates the need for manual data entry, saving a significant amount of time and reducing administrative burdens. The parsed data can be easily sorted, filtered, and compared, enabling recruiters to shortlist candidates efficiently and make informed decisions.\n",
    "\n",
    "In summary, resume parsing technology acts as a valuable assistant for hiring professionals, making the resume screening process more efficient, accurate, and manageable. It simplifies the initial stages of recruitment, allowing recruiters to focus their time and energy on evaluating the most promising candidates and conducting more meaningful interactions during interviews and assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f67d15f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer-six in c:\\users\\admin\\anaconda3\\lib\\site-packages (20221105)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pdfminer-six) (41.0.5)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pdfminer-six) (2.0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer-six) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six) (2.20)\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.11.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (58.0.4)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.62.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.20.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.26.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.4)\n",
      "Requirement already satisfied: pydantic-core==2.14.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.1)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Gauri Desai \n",
      "\n",
      "LinkedIn | https://www.linkedin.com/in/gauri-desai-19a423206 \n",
      "\n",
      "Email  | gaurid380@gmail.com \n",
      "\n",
      "Github \n",
      "\n",
      "| https://github.com/gaurid380/AIMLProjects \n",
      "\n",
      "Mobile | +91 9970161397 \n",
      "\n",
      "E-Portfolio Link | https://eportfolio.mygreatlearning.com/gauri-desai  Location: Pune \n",
      "\n",
      "Portfolio Link | https://sites.google.com/view/gauridesaiportfolio/home \n",
      "\n",
      "Summary \n",
      "16 years of software development experience. My area of expertise is in analysing customer requirements, \n",
      "planning, designing, development, deployment, testing, performance tuning and delivery of various projects with \n",
      "leading Java/J2EE/Android/IOS/Cloud technologies. \n",
      "I completed PG program in AIML from the University of Texas and now eager to apply AI ML technology to real \n",
      "world business problems.Highly-motivated with Machine Learning and AI capabilities so looking for opportunities \n",
      "where AIML involved. \n",
      "\n",
      "Technical Skills \n",
      "\n",
      "❖  Statistical  Modelling : EDA, Predictive Analytics, Hypothesis testing, Outlier Detection & Treatment, \n",
      "\n",
      "Missing Value Imputations, Feature Selection, Feature Engineering. \n",
      "\n",
      "❖  Machine Learning: Regression, Logistic Regression, Support Vector Machines (SVM), Ensemble \n",
      "\n",
      "Techniques and Random Forests, PCA and Dimensionality  Reduction, Clustering, Recommendation \n",
      "System \n",
      "\n",
      "❖  Deep Learning: Neural Network (ANN), Recurrent Neural Network (RNN),Mask RCNN, Long – Short \n",
      "\n",
      "Term Memory Network(LSTM), Transfer learning models, TensorFlow and Keras \n",
      "\n",
      "❖  NLP: Text Data Cleaning Methods, TF-IDF, Word2Vec,Stemming,Lemmatization, Named Entity \n",
      "\n",
      "Recognition, POS Tagging, NLTK, Spacy \n",
      "\n",
      "❖  Computer Vision: CNN Architectures, Classification, Object Detection, TensorFlow API \n",
      "❖  Data Visualization Tools: Grafana ,Tableau, Matplotlib, Plotly, Seaborn, TensorBoard, Power BI \n",
      "❖  Web Technologies: HTML, JavaScript, jQuery, Ajax, CSS , Django, Flask, Angular JS \n",
      "❖  Python Libraries: TensorFlow, NumPy, Pandas, Matplotlib, Keras, Scikit-learn, Scrapy, BeautifulSoup, \n",
      "\n",
      "Tkinter \n",
      "\n",
      "❖  Python IDE and Code Editor: PyCharm, Visual Studio Code,  Jupyter Notebook \n",
      "❖  MLOps: MLFlow, KubeFlow, AWS SageMaker, Evidently AI, DVC, CI/CD Pipeline deployment  on \n",
      "\n",
      "AWS, Azure and GCP clouds \n",
      "\n",
      "❖  Container Orchestration Tools: Docker \n",
      "❖  Generated AI Tools: ChatGPT, DALL-E, Midjourney, Bard, Bing, FlowGPT, AIPRM & Merlin browser \n",
      "\n",
      "ext \n",
      "\n",
      "❖  Cloud Technologies: AWS(S3, EC2, CloudWatch, AWS Elastic Beanstalk, RDS),Azure and GCP \n",
      "\n",
      "CI/CD pipelines, Heroku \n",
      "\n",
      "❖  Databases: Oracle 8i/9i/10g, SQL Server 2005, DB2, SQLite, DynamoDB, InfluxDb, MongoDB, RDS \n",
      "\n",
      "❖ \n",
      "\n",
      "IOT technologies: Arduino, ESP32, NodeMCU ESP8266, C#, Docklight \n",
      "\n",
      "❖  Version Control Tool: Git \n",
      "❖  Mobile Technologies: Android , Angular 8 , Flutter \n",
      "❖  Project Management Tool: JIRA, Bitbucket \n",
      "❖  Collaboration tool: Confluence  \n",
      "❖  Java Technologies/ Certifications: Java, Hibernate, Spring, SpringBoot, JSF, SCJP 1.5 Certification \n",
      "\n",
      "and SCWCD 1.5 Certification, ITIL Certification, AWS Cloud Practitioner Certification \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\f",
      "Academic Projects Delivered in AIML PG Program \n",
      "\n",
      "❖  Pneumonia Detection(Capstone Project) - Locate the position of inflammation in an image using \n",
      "Computer Vision, EDA, TensorBoard, Keras, TensorFlow, CNN-RESNET, Inception, UNet, SSD \n",
      "MobileNet, RCNN, Mask  RCNN, Adam, Optimizer, Clahe and Histogram Normalization to produce \n",
      "sharpen image. \n",
      "\n",
      "❖  Face Detection: Used MobileNet and on top of it added all the UNET layers to locate position of face in \n",
      "\n",
      "an image. \n",
      "\n",
      "Image Classification: Image classifier plant species using CNN. \n",
      "\n",
      "❖  Face Recognition: Pre-trained VGG face, PCA and SVM to recognize the faces. \n",
      "❖ \n",
      "❖  Semi – rule based chatbot: Built chatbot using NLTK and chatbot using DialogFlow \n",
      "❖  Sentiment Analysis: Recognize customer's sentiments based on their reviews in the IMDB database \n",
      "\n",
      "using CNN. \n",
      "\n",
      "Professional Experience: 16 Years \n",
      "\n",
      "❖  Acorn Pvt Ltd | Freelance Developer Duration: April 2020- April 2022 \n",
      "❖  Accenture | Mobility Lead Duration: December 2009- March 2019 \n",
      "❖  Glodyne Technoserve Pvt Ltd | Senior Software Engineer  Duration: January 2009- November 2009  \n",
      "❖  Persistent System Pvt Ltd | Software Engineer Duration: November 2006- December 2008  \n",
      "❖  YCS India Pvt Ltd | Software Developer  Duration: August 2004- August 2006  \n",
      "\n",
      "Responsibilities: \n",
      "\n",
      "❖  Engaged in development, support and enhancement of multiple Pharma applications \n",
      "❖  Maintained complex environment deployments end to end \n",
      "❖  Managed deliverables according to timeline and scope \n",
      "❖  Coordinated with client ,onshore team and off shore team \n",
      "❖  Attended client meeting to understand the functionality and get it developed along with team members \n",
      "❖  Engaged in load testing and CMMI related activities \n",
      "\n",
      "❖  Visited Madrid (Spain) for requirement gathering and prototype development \n",
      "❖  Fixed customer problems / concerns as quickly as possible, provided correct solution \n",
      "❖  Engaged in DB performance Tuning and Performance Tuning of Application \n",
      "❖  Generated project plans that meet client needs with company goals \n",
      "❖  Co-ordinated with team managers and team members throughout all stages of project \n",
      "\n",
      "❖  Conducted training session to team members for knowledge transfer \n",
      "❖  Participated in cross team co-ordination to resolve various issues during packaging, deployment and \n",
      "\n",
      "bringing up the application \n",
      "\n",
      "❖  Communicating proactively with clients, Negotiating timelines with them. Let them know the project \n",
      "\n",
      "milestones \n",
      "Identify right candidate profiles for open position and participate in recruitment process \n",
      "\n",
      "❖ \n",
      "❖  Played role of Change Manager \n",
      "❖  Accomplished POC for processing JSON file which will come from queue and store it to InfluxDB and \n",
      "\n",
      "analyse multiple time-series store data using Grafana and visualization of data \n",
      "\n",
      "❖  Designed and developed Water Meter Configuration Desktop App. Engaged in development and end to \n",
      "\n",
      "end testing of the desktop app. Used C#, Arduino, ESP32 \n",
      "\n",
      "❖  Developed TaxiMeter adapter USB driver APIs with Android interface. Engaged in development and end \n",
      "\n",
      "to end testing of the android app with TaxiMeter adapter USB driver APIs.(Android app communication \n",
      "with FTDI device) \n",
      "\n",
      "❖  Borewell App development using Android studio 3.3, Docklight, flutter technologies.(IOT based android \n",
      "\n",
      "app) and upload the app on Playstore \n",
      "Implemented CI/CD ML pipelines on AWS, Azure and GCP clouds. \n",
      "\n",
      "❖ \n",
      "\n",
      "Academic Qualification  \n",
      "\n",
      "❖  Post Graduate Program for Artificial Intelligence and Machine Learning from Great Lakes Institute and \n",
      "\n",
      "UAT Austin in 2022 \n",
      "\n",
      "❖  Bachelor’s in Computer Science, passed in June 2003 from Fr.C.R.I.T, Mumbai University \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\f",
      "Professional Certification \n",
      "\n",
      "❖  Post Graduate Program for Artificial Intelligence and Machine Learning from Great Lakes Institute and \n",
      "\n",
      "UAT Austin in April 2022 \n",
      "\n",
      "❖  Completed AWS Cloud Practitioner certification in March 2023 \n",
      "\n",
      "Training Done  \n",
      "\n",
      "❖  AWS Accreditation : Basic Foundation in Accenture \n",
      "❖  Udemy: IOT, Alexa, Basics of Python, Influxdb, AWS Essentials , Flutter, Flask, Django, React \n",
      "❖  Great Learning: AI and ML, IOT, Cloud Computing, AWS SageMaker, Devops, Docker, PowerBI, \n",
      "\n",
      "Tableau \n",
      "\n",
      "Awards & Merits \n",
      "\n",
      "❖  Achieved top 10% in Hackathon(The aim of the problem is to predict whether an applicant will default on \n",
      "\n",
      "the home loan payment or not.) AIML competition in Dec 2021. \n",
      "\n",
      "❖  Rewarded and appreciated by Client (Madrid) in June 2012 \n",
      "❖  Awarded Star Performer in Jan 2010 by Accenture \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer-six\n",
    "!python -m spacy download en_core_web_sm\n",
    "from pdfminer.high_level import extract_text\n",
    " \n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    print(extract_text_from_pdf(r\"F:\\interview\\GauriDesaiResume18-11-2023.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ff19bb",
   "metadata": {},
   "source": [
    "# Exctracting Name from Resume:\n",
    "\n",
    "The code snippet demonstrates a function that extracts text from a PDF file using pdfminer library. It then utilizes a regular expression pattern to extract a potential name from the extracted text. If a name is found, it is printed; otherwise, a \"Name not found\" message is displayed. This code can be used as a starting point for resume parsing tasks to extract names from resumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82355882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Gauri Desai\n"
     ]
    }
   ],
   "source": [
    "import pdfminer\n",
    "import re\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "def extract_name_from_resume(text):\n",
    "    name = None\n",
    "\n",
    "    # Use regex pattern to find a potential name\n",
    "    pattern = r\"(\\b[A-Z][a-z]+\\b)\\s(\\b[A-Z][a-z]+\\b)\"\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        name = match.group()\n",
    "\n",
    "    return name\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf(r\"F:\\interview\\GauriDesaiResume18-11-2023.pdf\")\n",
    "    name = extract_name_from_resume(text)\n",
    "\n",
    "    if name:\n",
    "        print(\"Name:\", name)\n",
    "    else:\n",
    "        print(\"Name not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c97198",
   "metadata": {},
   "source": [
    "# Exctract Contact Number:\n",
    "\n",
    "The provided code snippet defines a function to extract text from a PDF file using pdfminer. It also includes another function to extract a potential contact number from the extracted text using a regular expression pattern. The code then calls these functions to extract the contact number from a specific resume file. If a contact number is found, it is printed; otherwise, a \"Contact Number not found\" message is displayed. This code can be used as a starting point for extracting contact numbers from resumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a1ea1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact Number: 91 9970161397\n"
     ]
    }
   ],
   "source": [
    " def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "def extract_contact_number_from_resume(text):\n",
    "    contact_number = None\n",
    "\n",
    "    # Use regex pattern to find a potential contact number\n",
    "    pattern = r\"\\b(?:\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b\"\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        contact_number = match.group()\n",
    "\n",
    "    return contact_number\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf(r\"F:\\interview\\GauriDesaiResume18-11-2023.pdf\")\n",
    "    contact_number = extract_contact_number_from_resume(text)\n",
    "\n",
    "    if contact_number:\n",
    "        print(\"Contact Number:\", contact_number)\n",
    "    else:\n",
    "        print(\"Contact Number not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3499079",
   "metadata": {},
   "source": [
    "# Exctract Email Id : \n",
    "\n",
    "The provided code snippet defines a function extract_text_from_pdf() to extract text from a PDF file using pdfminer. It also includes another function extract_email_from_resume() to extract a potential email address from the extracted text using a regular expression pattern.\n",
    "\n",
    "The code then calls these functions to extract the email address from a specific resume file. If an email address is found, it is printed as \"Email: [email address]\"; otherwise, a \"Email not found\" message is displayed.\n",
    "\n",
    "This code can be used as a starting point for extracting email addresses from resumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f0f6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email: gaurid380@gmail.com\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "def extract_email_from_resume(text):\n",
    "    email = None\n",
    "\n",
    "    # Use regex pattern to find a potential email address\n",
    "    pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\"\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        email = match.group()\n",
    "\n",
    "    return email\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf(r\"F:\\interview\\GauriDesaiResume18-11-2023.pdf\")\n",
    "    email = extract_email_from_resume(text)\n",
    "\n",
    "    if email:\n",
    "        print(\"Email:\", email)\n",
    "    else:\n",
    "        print(\"Email not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3540ffd7",
   "metadata": {},
   "source": [
    "# Exctracting Skills:\n",
    "\n",
    "The provided code snippet includes a function extract_text_from_pdf() that extracts text from a PDF file using pdfminer. Additionally, it defines a function extract_skills_from_resume() that takes the extracted text and a list of predefined skills as input.\n",
    "\n",
    "The extract_skills_from_resume() function searches for each skill in the provided list within the resume text using regular expressions. If a skill is found, it is added to the skills list. Finally, the function returns the list of extracted skills.\n",
    "\n",
    "In the code's main section, the extract_text_from_pdf() function is called to extract the text from a specific resume file. A predefined list of skills is defined, and the extract_skills_from_resume() function is invoked with the extracted text and skills list as arguments. The extracted skills are then printed as \"Skills: [extracted skills]\" if any skills are found, otherwise, a \"No skills found\" message is displayed.\n",
    "\n",
    "This code can be utilized to extract skills from resumes by providing a list of predefined skills and the resume text. It serves as a basic framework for skill extraction and can be extended or customized to meet specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90a413b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills: ['Python', 'Machine Learning', 'Communication', 'Project Management', 'Deep Learning', 'SQL', 'Tableau', 'AI']\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "def extract_skills_from_resume(text, skills_list):\n",
    "    skills = []\n",
    "\n",
    "    # Search for skills in the resume text\n",
    "    for skill in skills_list:\n",
    "        pattern = r\"\\b{}\\b\".format(re.escape(skill))\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            skills.append(skill)\n",
    "\n",
    "    return skills\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf(r\"F:\\interview\\GauriDesaiResume18-11-2023.pdf\")\n",
    "\n",
    "    # List of predefined skills\n",
    "    skills_list = ['Python', 'Data Analysis', 'Machine Learning', 'Communication', 'Project Management', 'Deep Learning', 'SQL', 'Tableau','AI']\n",
    "\n",
    "    extracted_skills = extract_skills_from_resume(text, skills_list)\n",
    "\n",
    "    if extracted_skills:\n",
    "        print(\"Skills:\", extracted_skills)\n",
    "    else:\n",
    "        print(\"No skills found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44df8258",
   "metadata": {},
   "source": [
    "# Exctracting Education:\n",
    "\n",
    "The provided code snippet consists of a function extract_text_from_pdf() that extracts text from a PDF file using the pdfminer library. Additionally, it includes a function extract_education_from_resume() that takes the extracted text as input.\n",
    "\n",
    "The extract_education_from_resume() function utilizes a regular expression pattern to search for education information in the resume text. The pattern is designed to match various education degrees such as BSc, B.Tech, M.Tech, Ph.D., Bachelor's, Master's, and Ph.D., followed by the corresponding field of study.\n",
    "\n",
    "Within the code's main section, the extract_text_from_pdf() function is invoked to extract the text from a specific resume file. Then, the extract_education_from_resume() function is called with the extracted text as an argument. If any education information is found, it is appended to the education list. Finally, the list of extracted education details is printed as \"Education: [extracted_education]\" if education information is found. Otherwise, a \"No education information found\" message is displayed.\n",
    "\n",
    "This code provides a basic framework for extracting education information from resumes using regular expressions. It can be further customized or expanded to handle additional patterns or extract more specific details related to education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72ff2cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No education information found\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "def extract_education_from_resume(text):\n",
    "    education = []\n",
    "\n",
    "    # Use regex pattern to find education information\n",
    "    pattern = r\"(?i)(?:(?:Bachelor(?:'s)?|B\\.S\\.|B\\.A\\.|Master|M\\.S\\.|M\\.A\\.|Ph\\.D\\.)\\s(?:[A-Za-z]+\\s)*[A-Za-z]+)\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    for match in matches:\n",
    "        education.append(match.strip())\n",
    "\n",
    "    return education\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf(r\"F:\\interview\\GauriDesaiResume18-11-2023.pdf\")\n",
    "\n",
    "    extracted_education = extract_education_from_resume(text)\n",
    "    if extracted_education:\n",
    "        print(\"Education:\", extracted_education)\n",
    "    else:\n",
    "        print(\"No education information found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a80890c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No education information found\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "def extract_education_from_resume(text):\n",
    "    education = []\n",
    "\n",
    "    # Use regex pattern to find education information\n",
    "    pattern = r\"(?i)(?:Bsc|\\bB\\.\\w+|\\bM\\.\\w+|\\bPh\\.D\\.\\w+|\\bBachelor(?:'s)?|\\bMaster(?:'s)?|\\bPh\\.D)\\s(?:\\w+\\s)*\\w+\"\n",
    "    \n",
    "    matches = re.findall(pattern, text)\n",
    "    for match in matches:\n",
    "        education.append(match.strip())\n",
    "\n",
    "    return education\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf(r\"F:\\interview\\GauriDesaiResume18-11-2023.pdf\")\n",
    "\n",
    "    extracted_education = extract_education_from_resume(text)\n",
    "    if extracted_education:\n",
    "        print(\"Education:\", extracted_education)\n",
    "    else:\n",
    "        print(\"No education information found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd98bce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.20.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (58.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.26.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.11.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.62.3)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.8.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.1)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Data Science Education: ['AIML', 'AIML PG Program']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_data_science_education(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    education = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'ORG' and 'AIML' in ent.text:\n",
    "            education.append(ent.text)\n",
    "\n",
    "    return education\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf(r\"F:\\interview\\GauriDesaiResume18-11-2023.pdf\")\n",
    "\n",
    "    extracted_education = extract_data_science_education(text)\n",
    "    if extracted_education:\n",
    "        print(\"Data Science Education:\", extracted_education)\n",
    "    else:\n",
    "        print(\"No data science education found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dedc044f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University: I completed PG program in AIML from the University of Texas and now eager to apply AI ML technology to real\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_university_name(text):\n",
    "    lines = text.split('\\n')\n",
    "    college_pattern = r\"(?i).*University.*\"\n",
    "    for line in lines:\n",
    "        if re.match(college_pattern, line):\n",
    "            return line.strip()\n",
    "    return None\n",
    "\n",
    "# Example usage:\n",
    "    text = extract_text_from_pdf(r\"F:\\interview\\GauriDesaiResume13-09-2023.pdf\")\n",
    "\n",
    "\n",
    "university_name = extract_university_name(text)\n",
    "if university_name:\n",
    "    print(\"University:\", university_name)\n",
    "else:\n",
    "    print(\"University name not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb81c782",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "In conclusion, the provided code snippets demonstrate a basic implementation of a resume parser. Each code snippet focuses on extracting specific information from a resume, such as name, contact number, email, skills, and education.\n",
    "\n",
    "The resume parser utilizes various techniques, including regular expressions and text extraction from PDF files. It showcases how these techniques can be applied to automate the extraction of important details from resumes.\n",
    "\n",
    "However, it's important to note that the code snippets provide a starting point and can be further enhanced and customized based on specific requirements. For example, additional patterns or algorithms can be implemented to improve the accuracy of information extraction.\n",
    "\n",
    "Resume parsing plays a vital role in automating the initial screening process for job applications. By extracting key details from resumes, it saves time and effort for recruiters and allows for efficient filtering of candidates.\n",
    "\n",
    "As technology advances, resume parsing algorithms can be further refined to handle more complex resume formats, languages, and diverse information extraction requirements. This will help in building more sophisticated and accurate resume parsing systems.\n",
    "\n",
    "Overall, the provided code snippets serve as a foundation for developing a resume parser and demonstrate the potential of automating the extraction of essential information from resumes, streamlining the recruitment process, and improving efficiency in candidate evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085324be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
